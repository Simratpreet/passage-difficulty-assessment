{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\nWe have a dataset containing excerpts of different pasaages and target column indicating the difficult of the passage i.e the reading ease. We need to predict the reading ease of unknown passages in test data.\n\n## Approach Followed:\nHere we follow below steps to solve this problem -\n* Combine the train and test excerpts to preprocess them at once\n* Apply preprocessing to these texts\n* Split the data into train and test sets using sklearn\n* Use Google Word2Vec pretrained embeddings to get an average vector for each excerpt text as an input to tree based models\n* Run XGBoost and LightGBM models with embedding vectors as an input and reading ease as target variable\n* Pick one of these basis rmse to optimize using Hyperopt\n* Run the Hyperopt trials to get best hyperparameters for LGBM\n* Make predictions with optimized LGBM","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\n\ntrain_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\nprint(train_df.shape)\nprint(test_df.shape)\ntrain_df.head(5)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-22T06:55:41.869794Z","iopub.execute_input":"2021-06-22T06:55:41.870302Z","iopub.status.idle":"2021-06-22T06:55:45.655022Z","shell.execute_reply.started":"2021-06-22T06:55:41.870204Z","shell.execute_reply":"2021-06-22T06:55:45.654004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess the Data","metadata":{"execution":{"iopub.status.busy":"2021-06-22T06:55:45.77941Z","iopub.execute_input":"2021-06-22T06:55:45.779837Z","iopub.status.idle":"2021-06-22T06:55:45.809353Z","shell.execute_reply.started":"2021-06-22T06:55:45.779793Z","shell.execute_reply":"2021-06-22T06:55:45.808348Z"}}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nlemma = WordNetLemmatizer()\ndef preprocess(text):\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text = text.lower()\n    words = text.split()\n    words = [lemma.lemmatize(word) for word in words if word not in stopwords.words('english')]\n    return words","metadata":{"execution":{"iopub.status.busy":"2021-06-22T06:55:45.810744Z","iopub.execute_input":"2021-06-22T06:55:45.811185Z","iopub.status.idle":"2021-06-22T06:55:45.817052Z","shell.execute_reply.started":"2021-06-22T06:55:45.811116Z","shell.execute_reply":"2021-06-22T06:55:45.815864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"excerpt_text = train_df['excerpt'].append(test_df['excerpt'])\nexcerpt_text = excerpt_text.apply(lambda x: preprocess(x))\nexcerpt_text.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T06:55:45.819213Z","iopub.execute_input":"2021-06-22T06:55:45.81972Z","iopub.status.idle":"2021-06-22T06:56:49.964054Z","shell.execute_reply.started":"2021-06-22T06:55:45.819676Z","shell.execute_reply":"2021-06-22T06:56:49.963026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(excerpt_text.head(train_df.shape[0]), train_df['target'])\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T06:56:49.965667Z","iopub.execute_input":"2021-06-22T06:56:49.965948Z","iopub.status.idle":"2021-06-22T06:56:49.974254Z","shell.execute_reply.started":"2021-06-22T06:56:49.965922Z","shell.execute_reply":"2021-06-22T06:56:49.97321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use Google Word2Vec","metadata":{}},{"cell_type":"code","source":"embedding = gensim.models.KeyedVectors.load_word2vec_format('../input/word2vec-google/GoogleNews-vectors-negative300.bin', binary=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2021-06-22T06:56:49.975393Z","iopub.execute_input":"2021-06-22T06:56:49.975683Z","iopub.status.idle":"2021-06-22T06:56:49.985315Z","shell.execute_reply.started":"2021-06-22T06:56:49.975656Z","shell.execute_reply":"2021-06-22T06:56:49.984503Z"}}},{"cell_type":"code","source":"# this function computes the averaged vector for each sentence\ndef get_averaged_vectors(embedding, X, size=300):\n    vectors = []\n    for row in X.values:\n        feature_vec = np.zeros(size, dtype='float32')\n        i = 0\n        for token in row:\n            try:\n                feature_vec = np.add(feature_vec, embedding[token])\n                i += 1\n            except:\n                pass\n        feature_vec = np.divide(feature_vec, i) \n        vectors.append(feature_vec)\n    \n    return vectors","metadata":{"execution":{"iopub.status.busy":"2021-06-22T19:12:19.450056Z","iopub.execute_input":"2021-06-22T19:12:19.450592Z","iopub.status.idle":"2021-06-22T19:12:19.456851Z","shell.execute_reply.started":"2021-06-22T19:12:19.450471Z","shell.execute_reply":"2021-06-22T19:12:19.455421Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_vectors = get_averaged_vectors(embedding, X_train, 300)\ntest_vectors = get_averaged_vectors(embedding, X_test, 300)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T07:13:51.038277Z","iopub.execute_input":"2021-06-21T07:13:51.038973Z","iopub.status.idle":"2021-06-21T07:13:52.1376Z","shell.execute_reply.started":"2021-06-21T07:13:51.038933Z","shell.execute_reply":"2021-06-21T07:13:52.136489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run XGB and LGBM","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(n_estimators=100, n_jobs=-1, learning_rate=0.01)\nxgb.fit(pd.DataFrame(train_vectors), y_train)\ny_test_pred = xgb.predict(pd.DataFrame(test_vectors))\nprint('Test set RMSE %s' % np.sqrt(mean_squared_error(y_test, y_test_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:05.483092Z","iopub.execute_input":"2021-06-20T15:03:05.483441Z","iopub.status.idle":"2021-06-20T15:03:05.487377Z","shell.execute_reply.started":"2021-06-20T15:03:05.483392Z","shell.execute_reply":"2021-06-20T15:03:05.48652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\n\nlgb = LGBMRegressor(n_estimators=100, n_jobs=-1, learning_rate=0.01)\nlgb.fit(pd.DataFrame(train_vectors), y_train)\ny_test_pred = lgb.predict(pd.DataFrame(test_vectors))\nprint('Test set RMSE %s' % np.sqrt(mean_squared_error(y_test, y_test_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T06:40:06.997907Z","iopub.execute_input":"2021-06-21T06:40:06.998339Z","iopub.status.idle":"2021-06-21T06:40:07.808207Z","shell.execute_reply.started":"2021-06-21T06:40:06.998293Z","shell.execute_reply":"2021-06-21T06:40:07.807294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optimize using Hyperopt","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold, cross_val_score\n\nrandom_state = 42\nnum_folds = 5\nn_iter = 50\nkf = KFold(n_splits=num_folds)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T06:40:25.54659Z","iopub.execute_input":"2021-06-21T06:40:25.547436Z","iopub.status.idle":"2021-06-21T06:40:25.551806Z","shell.execute_reply.started":"2021-06-21T06:40:25.547389Z","shell.execute_reply":"2021-06-21T06:40:25.551063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hyperopt import fmin, tpe, hp, anneal, Trials\ndef gb_mse_cv(params, random_state=random_state, cv=kf, X=pd.DataFrame(train_vectors), y=y_train):\n    # the function gets a set of variable parameters in \"param\"\n    params = {'n_estimators': int(params['n_estimators']), \n              'max_depth': int(params['max_depth']), \n             'learning_rate': params['learning_rate']}\n    \n    # we use this params to create a new LGBM Regressor\n    model = LGBMRegressor(random_state=random_state, **params)\n    \n    # and then conduct the cross validation with the same folds as before\n    score = -cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\", n_jobs=-1).mean()\n\n    return score\n\n%%time\n\n# possible values of parameters\nspace={'n_estimators': hp.quniform('n_estimators', 100, 2000, 1),\n       'max_depth' : hp.quniform('max_depth', 2, 20, 1),\n       'learning_rate': hp.loguniform('learning_rate', -5, 0)\n      }\n\n# trials will contain logging information\ntrials = Trials()\n\nbest=fmin(fn=gb_mse_cv, # function to optimize\n          space=space, \n          algo=tpe.suggest, # optimization algorithm, hyperotp will select its parameters automatically\n          max_evals=n_iter, # maximum number of iterations\n          trials=trials, # logging\n          rstate=np.random.RandomState(random_state) # fixing random state for the reproducibility\n         )\n\n# computing the score on the test set\nmodel = LGBMRegressor(random_state=random_state, n_estimators=int(best['n_estimators']),\n                      max_depth=int(best['max_depth']),learning_rate=best['learning_rate'])\nmodel.fit(pd.DataFrame(train_vectors), y_train)","metadata":{"execution":{"iopub.status.busy":"2021-06-21T06:40:26.199339Z","iopub.execute_input":"2021-06-21T06:40:26.200098Z","iopub.status.idle":"2021-06-21T06:40:26.310268Z","shell.execute_reply.started":"2021-06-21T06:40:26.200057Z","shell.execute_reply":"2021-06-21T06:40:26.309218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Make Predictions","metadata":{}},{"cell_type":"code","source":"lgb = LGBMRegressor(learning_rate=0.009507284992946358, max_depth=20,\n              n_estimators=1617, random_state=42)\n\nlgb.fit(pd.DataFrame(train_vectors), y_train)\ny_test_pred = lgb.predict(pd.DataFrame(test_vectors))\nprint('Test set RMSE %s' % np.sqrt(mean_squared_error(y_test, y_test_pred)))","metadata":{"execution":{"iopub.status.busy":"2021-06-21T07:13:57.407656Z","iopub.execute_input":"2021-06-21T07:13:57.408092Z","iopub.status.idle":"2021-06-21T07:14:27.488445Z","shell.execute_reply.started":"2021-06-21T07:13:57.408058Z","shell.execute_reply":"2021-06-21T07:14:27.487586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test_set_vectors = get_averaged_vectors(excerpt_text[-test_df.shape[0]:])\npredictions = lgb.predict(pd.DataFrame(pred_test_set_vectors))\nsample_submission = pd.DataFrame(list(zip(test_df['id'], predictions)), columns=['id', 'target'])\nsample_submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-20T15:03:39.334714Z","iopub.execute_input":"2021-06-20T15:03:39.335175Z","iopub.status.idle":"2021-06-20T15:03:39.343239Z","shell.execute_reply.started":"2021-06-20T15:03:39.335139Z","shell.execute_reply":"2021-06-20T15:03:39.342467Z"},"trusted":true},"execution_count":null,"outputs":[]}]}