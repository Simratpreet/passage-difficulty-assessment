{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem Statement\nWe have a dataset containing excerpts of different pasaages and target column indicating the difficult of the passage i.e the reading ease. We need to predict the reading ease of unknown passages in test data.\n\n## Approach Followed:\nHere we follow below steps to solve this problem -\n* Combine the train and test excerpts to preprocess them at once\n* Apply preprocessing to these texts\n* Prepare data for LSTM model by creating padded sequences of same length\n* Use Google Word2Vec pretrained embeddings to create an embedding matrix for the LSTM input layer\n* Run LSTM model on train data, keeping test split as validation set","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrain_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/train.csv')\ntest_df = pd.read_csv('/kaggle/input/commonlitreadabilityprize/test.csv')\nprint(train_df.shape)\nprint(test_df.shape)\ntrain_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T12:56:56.899290Z","iopub.execute_input":"2021-06-22T12:56:56.899853Z","iopub.status.idle":"2021-06-22T12:56:57.035041Z","shell.execute_reply.started":"2021-06-22T12:56:56.899740Z","shell.execute_reply":"2021-06-22T12:56:57.033668Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"(2834, 6)\n(7, 4)\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"          id url_legal license  \\\n0  c12129c31       NaN     NaN   \n1  85aa80a4c       NaN     NaN   \n2  b69ac6792       NaN     NaN   \n3  dd1000b26       NaN     NaN   \n4  37c1b32fb       NaN     NaN   \n\n                                             excerpt    target  standard_error  \n0  When the young people returned to the ballroom... -0.340259        0.464009  \n1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n3  And outside before the palace a great garden w... -1.054013        0.450007  \n4  Once upon a time there were Three Bears who li...  0.247197        0.510845  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>url_legal</th>\n      <th>license</th>\n      <th>excerpt</th>\n      <th>target</th>\n      <th>standard_error</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c12129c31</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>When the young people returned to the ballroom...</td>\n      <td>-0.340259</td>\n      <td>0.464009</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>85aa80a4c</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n      <td>-0.315372</td>\n      <td>0.480805</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b69ac6792</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>As Roger had predicted, the snow departed as q...</td>\n      <td>-0.580118</td>\n      <td>0.476676</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dd1000b26</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>And outside before the palace a great garden w...</td>\n      <td>-1.054013</td>\n      <td>0.450007</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37c1b32fb</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Once upon a time there were Three Bears who li...</td>\n      <td>0.247197</td>\n      <td>0.510845</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Clean the excerpt text\n1. Remove all characters apart from alphabets\n2. Lowercase the text\n3. Lemmatize the text data","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nlemma = WordNetLemmatizer()\ndef preprocess(text):\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text = text.lower()\n    words = text.split()\n    words = [lemma.lemmatize(word) for word in words if word not in stopwords.words('english')]\n    return words","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:13:36.762480Z","iopub.execute_input":"2021-06-22T09:13:36.762989Z","iopub.status.idle":"2021-06-22T09:13:36.768972Z","shell.execute_reply.started":"2021-06-22T09:13:36.762953Z","shell.execute_reply":"2021-06-22T09:13:36.767933Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"excerpt_text = train_df['excerpt'].append(test_df['excerpt'])\nexcerpt_text = excerpt_text.apply(lambda x: preprocess(x))\nexcerpt_text.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:13:36.770533Z","iopub.execute_input":"2021-06-22T09:13:36.770991Z","iopub.status.idle":"2021-06-22T09:14:44.110614Z","shell.execute_reply.started":"2021-06-22T09:13:36.770943Z","shell.execute_reply":"2021-06-22T09:14:44.109327Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0    [young, people, returned, ballroom, presented,...\n1    [dinner, time, mr, fayre, somewhat, silent, ey...\n2    [roger, predicted, snow, departed, quickly, ca...\n3    [outside, palace, great, garden, walled, round...\n4    [upon, time, three, bear, lived, together, hou...\nName: excerpt, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# Tokenize the excerpt text data. This will assign unique integer to every unique word in excerpt text data\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(excerpt_text)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:14:44.112009Z","iopub.execute_input":"2021-06-22T09:14:44.112372Z","iopub.status.idle":"2021-06-22T09:14:46.093356Z","shell.execute_reply.started":"2021-06-22T09:14:44.112339Z","shell.execute_reply":"2021-06-22T09:14:46.092011Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Get total words in excerpt text. This will be used to create the embedding matrix of shape (vocab_size, Dimension(word_embedding))\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary size is {}\".format(vocab_size))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:14:46.094804Z","iopub.execute_input":"2021-06-22T09:14:46.095131Z","iopub.status.idle":"2021-06-22T09:14:46.100874Z","shell.execute_reply.started":"2021-06-22T09:14:46.095098Z","shell.execute_reply":"2021-06-22T09:14:46.099640Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Vocabulary size is 22597\n","output_type":"stream"}]},{"cell_type":"code","source":"sequences = tokenizer.texts_to_sequences(excerpt_text)\nprint(f\"There are {excerpt_text.shape[0]} excerpts and {len(sequences)} sequences\")\nprint(f\"Min sequence length is {min([len(s) for s in sequences])}\")\nprint(f\"Max sequence length is {max([len(s) for s in sequences])}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:14:46.104618Z","iopub.execute_input":"2021-06-22T09:14:46.105094Z","iopub.status.idle":"2021-06-22T09:14:46.260308Z","shell.execute_reply.started":"2021-06-22T09:14:46.105046Z","shell.execute_reply":"2021-06-22T09:14:46.259000Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"There are 2841 excerpts and 2841 sequences\nMin sequence length is 52\nMax sequence length is 135\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nsequences = pad_sequences(sequences, padding='post')\nprint(f\"Min sequence length is {min([len(s) for s in sequences])}\")\nprint(f\"Max sequence length is {max([len(s) for s in sequences])}\")\nprint(f\"Shape of sequences is {sequences.shape}\")","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:14:46.262966Z","iopub.execute_input":"2021-06-22T09:14:46.263444Z","iopub.status.idle":"2021-06-22T09:14:46.350792Z","shell.execute_reply.started":"2021-06-22T09:14:46.263390Z","shell.execute_reply":"2021-06-22T09:14:46.349737Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Min sequence length is 135\nMax sequence length is 135\nShape of sequences is (2841, 135)\n","output_type":"stream"}]},{"cell_type":"code","source":"embedding = gensim.models.KeyedVectors.load_word2vec_format('../input/word2vec-google/GoogleNews-vectors-negative300.bin', binary=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:14:46.352778Z","iopub.execute_input":"2021-06-22T09:14:46.353222Z","iopub.status.idle":"2021-06-22T09:15:33.103100Z","shell.execute_reply.started":"2021-06-22T09:14:46.353175Z","shell.execute_reply":"2021-06-22T09:15:33.102057Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, 300))\nfor word, i in tokenizer.word_index.items():\n    try:\n        embedding_vector = embedding.get_vector(word)\n    except:\n        embedding_vector = np.zeros((300,))\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nembedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:15:33.107169Z","iopub.execute_input":"2021-06-22T09:15:33.107521Z","iopub.status.idle":"2021-06-22T09:15:33.247969Z","shell.execute_reply.started":"2021-06-22T09:15:33.107487Z","shell.execute_reply":"2021-06-22T09:15:33.246914Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(22597, 300)"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras.layers import SimpleRNN, Embedding, Dense, LSTM\nfrom keras.initializers import Constant\n\nmodel = keras.Sequential()\nmodel.add(Embedding(vocab_size, 300, embeddings_initializer=Constant(embedding_matrix), input_length=135, trainable=False))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='linear'))\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:24:34.902718Z","iopub.execute_input":"2021-06-22T09:24:34.903112Z","iopub.status.idle":"2021-06-22T09:24:35.233091Z","shell.execute_reply.started":"2021-06-22T09:24:34.903077Z","shell.execute_reply":"2021-06-22T09:24:35.231905Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 135, 300)          6779100   \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 100)               160400    \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 101       \n=================================================================\nTotal params: 6,939,601\nTrainable params: 160,501\nNon-trainable params: 6,779,100\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntarget = train_df['target']\nX_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(sequences).head(train_df.shape[0]), target)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:15:33.699646Z","iopub.execute_input":"2021-06-22T09:15:33.699937Z","iopub.status.idle":"2021-06-22T09:15:33.709717Z","shell.execute_reply.started":"2021-06-22T09:15:33.699907Z","shell.execute_reply":"2021-06-22T09:15:33.708490Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(2125, 135)\n(2125,)\n(709, 135)\n(709,)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Training...\")\nhistory = model.fit(x=X_train, y=y_train, batch_size=16, epochs=15, validation_data=(X_test, y_test), verbose=2)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:24:37.448896Z","iopub.execute_input":"2021-06-22T09:24:37.449408Z","iopub.status.idle":"2021-06-22T09:27:42.729070Z","shell.execute_reply.started":"2021-06-22T09:24:37.449376Z","shell.execute_reply":"2021-06-22T09:27:42.728047Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Training...\nEpoch 1/15\n133/133 - 15s - loss: 0.9485 - mean_squared_error: 0.9485 - val_loss: 0.8839 - val_mean_squared_error: 0.8839\nEpoch 2/15\n133/133 - 12s - loss: 0.7402 - mean_squared_error: 0.7402 - val_loss: 0.7240 - val_mean_squared_error: 0.7240\nEpoch 3/15\n133/133 - 12s - loss: 0.8970 - mean_squared_error: 0.8970 - val_loss: 0.9136 - val_mean_squared_error: 0.9136\nEpoch 4/15\n133/133 - 12s - loss: 0.9655 - mean_squared_error: 0.9655 - val_loss: 1.0076 - val_mean_squared_error: 1.0076\nEpoch 5/15\n133/133 - 12s - loss: 0.9887 - mean_squared_error: 0.9887 - val_loss: 0.9937 - val_mean_squared_error: 0.9937\nEpoch 6/15\n133/133 - 12s - loss: 0.9879 - mean_squared_error: 0.9879 - val_loss: 1.0007 - val_mean_squared_error: 1.0007\nEpoch 7/15\n133/133 - 12s - loss: 0.9875 - mean_squared_error: 0.9875 - val_loss: 1.0096 - val_mean_squared_error: 1.0096\nEpoch 8/15\n133/133 - 12s - loss: 0.9882 - mean_squared_error: 0.9882 - val_loss: 1.0000 - val_mean_squared_error: 1.0000\nEpoch 9/15\n133/133 - 12s - loss: 0.9874 - mean_squared_error: 0.9874 - val_loss: 1.0283 - val_mean_squared_error: 1.0283\nEpoch 10/15\n133/133 - 12s - loss: 0.9926 - mean_squared_error: 0.9926 - val_loss: 0.9769 - val_mean_squared_error: 0.9769\nEpoch 11/15\n133/133 - 12s - loss: 0.8595 - mean_squared_error: 0.8595 - val_loss: 0.7745 - val_mean_squared_error: 0.7745\nEpoch 12/15\n133/133 - 12s - loss: 0.7153 - mean_squared_error: 0.7153 - val_loss: 0.6753 - val_mean_squared_error: 0.6753\nEpoch 13/15\n133/133 - 12s - loss: 0.6022 - mean_squared_error: 0.6022 - val_loss: 0.6070 - val_mean_squared_error: 0.6070\nEpoch 14/15\n133/133 - 12s - loss: 0.5262 - mean_squared_error: 0.5262 - val_loss: 0.5409 - val_mean_squared_error: 0.5409\nEpoch 15/15\n133/133 - 12s - loss: 0.4695 - mean_squared_error: 0.4695 - val_loss: 0.5079 - val_mean_squared_error: 0.5079\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\ny_pred = model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:29:14.138830Z","iopub.execute_input":"2021-06-22T09:29:14.139252Z","iopub.status.idle":"2021-06-22T09:29:15.748852Z","shell.execute_reply.started":"2021-06-22T09:29:14.139217Z","shell.execute_reply":"2021-06-22T09:29:15.747790Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"0.7126405569416674"},"metadata":{}}]},{"cell_type":"code","source":"pred = model.predict(pd.DataFrame(sequences).tail(test_df.shape[0])).reshape(test_df.shape[0])\nsample_submission = pd.DataFrame(list(zip(test_df['id'], pred)), columns=['id', 'target'])\nsample_submission","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:35:18.541065Z","iopub.execute_input":"2021-06-22T09:35:18.541482Z","iopub.status.idle":"2021-06-22T09:35:18.643529Z","shell.execute_reply.started":"2021-06-22T09:35:18.541430Z","shell.execute_reply":"2021-06-22T09:35:18.642362Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"          id    target\n0  c0f722661 -0.976133\n1  f0953f0a5 -0.757631\n2  0df072751 -1.031678\n3  04caf4e0c -1.752487\n4  0e63f8bea -2.273083\n5  12537fe78 -1.096488\n6  965e592c0  0.050508","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c0f722661</td>\n      <td>-0.976133</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>f0953f0a5</td>\n      <td>-0.757631</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0df072751</td>\n      <td>-1.031678</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>04caf4e0c</td>\n      <td>-1.752487</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0e63f8bea</td>\n      <td>-2.273083</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>12537fe78</td>\n      <td>-1.096488</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>965e592c0</td>\n      <td>0.050508</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample_submission.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-06-22T09:35:26.524943Z","iopub.execute_input":"2021-06-22T09:35:26.525332Z","iopub.status.idle":"2021-06-22T09:35:26.535564Z","shell.execute_reply.started":"2021-06-22T09:35:26.525298Z","shell.execute_reply":"2021-06-22T09:35:26.534455Z"},"trusted":true},"execution_count":35,"outputs":[]}]}